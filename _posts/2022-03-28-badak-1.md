---
title:  "딥러닝 신경망 이론 및 구현" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-03-28
last_modified_at: 2022-03-28
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 1. 딥러닝 신경망 이론 및 구현
## 1.1 퍼셉트론이란?

<u>퍼셉트론</u>은 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 여기서 말하는 신호랑 전류나 강물처럼 흐름이 있는 것을 상상하면 좋다. 전류가 전선을 타고 흐르는 전자를 내보내듯, 퍼셉트론 신호도 흐름을 만들고 정보를 앞으로 전달한다. 

## 1.2 퍼셉트론의 한계
위에서 서술한 퍼셉트론으로는 복잡한 함수도 표현할 수 있으며 그 예로 컴퓨터가 수행하는 복잡한 처리도 퍼셉트론으로 표현할 수 있다고 한다. 하지만 퍼셉트론을 이루는 가중치를 설정하는 작업은 여전히 사람의 수작업을 통해서 진행된다. 

## 1.3 신경망
위 퍼셉트론의 한계를 벗어나는 해결책으로 우리는 신경망을 사용한다. 신경망은 퍼셉트론과 달리 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 가진다.

### 신경망의 예
<center>

![img3-1](../img/badak/fig%203-1.png)

</center>

위 신경망은 모두 3층으로 구성이되지만 가중치를 갖는 층은 2개뿐이기 때문에 <u>2층 신경망</u>이라고 한다.

## 1.4 활성화 함수
활성화 함수란 이름과 같이 입력신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. 가중치가 달린 입력신호와 편항의 총합을 계산하고 이를 a라고 한다. 그리고 a를 활성화 함수 h를 통하여 y를 출력하는 흐름이며 예는 아래와 같다.

<center>

$a = b + w_1x_1 + w_1x_1$

$y = h(a)$

![img3-2](../img/badak/fig%203-4.png)

</center>

또한 보통의 퍼셉트론에서는 계단 함수를 활성화 함수로 채용하는데 신경망에서는 계단 함수가 아닌 다른 활성 함수를 사용한다. 보통의 신경망에서 쉽게 사용가능한 활성함수로써 Sigmoid와 Relu를 소개한다.

### Step function

<center>

$h(x)=
\begin{cases}
1,\;if\;x>0\\
0,\;else
\end{cases}$

![img3-3](../img/badak/fig%203-6.png)

</center>

### Sigmoid

<center>

$
h(x) = \frac{1}{1 + exp(-x)}
$


![img3-4](../img/badak/fig%203-7.png)

</center>

### Relu

<center>

$h(x)=
\begin{cases}
x,\;if\;x>0\\
0,\;else
\end{cases}$

![img3-5](../img/badak/fig%203-9.png)

</center>

위 함수들을 보았을 때 공통적인 성질은 <u>비선형 함수</u>라는 것이다. 신경망에서는 활성화 함수로 선형 함수를 사용해서는 안되기 때문이다. 그 이유로는 선형 함수를 사용하여 은닉층을 늘려도 은닉층이 없는 네트워크와 똑같은 기능을 할 수 있다는 데에 있다. 간단한 예를들어 설명하자면 h(x) = cx 라는 활성 함수를 사용하고 3층짜리 신경망을 설계한다면 y = h(h(h(x))) 이며 값은 y = c^3x가 나올 것이고 y = ax, a=c^3 이와 같이 1층 짜리 신경망으로도 구현할 수 있다. 그러므로 층을 쌓으므로써 혜택을 얻고싶다면 활성함수로는 비선형 함수를 사용하여야 한다.

***Activation_Func.py***
```python
def step_function(x):
	y = x > 0
	return y.astype(np.int)

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

def Relu(x):
	return np.maximum(0, x)
```

## 1.5 출력층 설계
신경망은 분류와 회귀 모두에 이용할 수 있다. 다만 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라지게 되고 일반적으로는 회귀 문제에서는 항등 함수를, 분류에서는 소프트맥스 함수를 사용한다.

### 항등 함수
항등 함수는 입력을 그대로 출력한다. 입력과 출력이 항상 같다는 뜻의 항등이며 출력층에서 항등 함수를 사용한다면 입력 신호가 그대로 출력 신호가 된다.

### 소프트맥스

$$
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n} a_i}
$$

위의 수식과 같이 소프트맥스는 분자가 입력 신호의 지수함수, 분모는 모든 입력신호의 지수 함수의 함으로 구성이된다. 또한 소프트맥스 출력의 총합은 1이다. 출력의 총합이 1이 된다는 것은 소프트 맥스 함수의 중요한 성질이며 이러한 성질로 인해 소프트맥스 함수의 출력을 <u>확률</u>로써 해석할 수 있게된다.

***Out_Func.py***
```python
def identity_func(x):
	return x

def softmax(x):
	c = np.max(x)
	exp_x = np.exp(x - c)
	sum_exp_x = np.sum(exp_x)
	y = exp_x / sum_exp_x

	return y

```

출력층의 누련 수는 풀려는 문제에 맞게 적절히 정하면 된다. 분류에서는 분류하고싶은 클래스 수로 설정하는 것이 일반적이며 예를들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 아래의 그림과 같이 출력층의 개수를 10개로 정하면 된다.

<center>

![img3-6](../img/badak/fig%203.23.png)

</center>

## 1.6 데이터 셋
현재 공부하고 있는 책인 <u>밑바닥부터 시작하는 딥러닝</u>에서는 MNIST라는 손 글씨 데이터를 사용하고 있으며 앞으로의 딥러닝 관련 포스팅에서 사용할 데이터 셋 또한 MNIST를 사용할 예정이다. MNIST 데이터셋은 0부터 9까지의 숫자 이미지로 구성이되며 훈련 이미지 60,000장, 시험 이미지 10,000장으로 구성된다. 일반적으로 훈련 이미지들을 사용하여 모델을 학습하고 학습한 모델로 시험 이미지들을 얼마나 정확하게 분류하는지를 평가한다.

![img3-7](../img/badak/fig%203-24.png)

MNIST의 이미지 데이터는 28x28 크기의 1채널 이미지이며 각 픽셀은 0~255 값을 가진다. 또한 각 이미지에는 그 이미지가 실제 의미하는 숫자가 레이블링이 되어있다. 아래 링크는 MNIST 데이터 셋을 받을 수 있는 링크이다.

Link-1: http://yann.lecun.com/exdb/mnist/


## 1.7 신경망 구현
앞에서의 과정을 통하여 배운 과정으로 순전파를 구현해보고자 한다. 현재 포스팅에서는 학습을 시키는 과정은 배우지 않았기 때문에 미리 학습시켰던 가중치를 사용하였다.

***Neural_Network.py***
```python
import sys, os
sys.path.append(os.pardir)
from dataset.mnist import load_mnist
import numpy as np
import pickle
from Out_Func import softmax
from Activation_Func import sigmoid


def predict(network, x):
    W1, W2, W3 = network["W1"], network["W2"], network["W3"]
    b1, b2, b3 = network["b1"], network["b2"], network["b3"]

    a1 = x @ W1 + b1
    z1 = sigmoid(a1)
    a2 = z1 @ W2 + b2
    z2 = sigmoid(a2)
    a3 = z2 @ W3 + b3
    y = softmax(a3)
    return y

def accuracy(x, t, network,batch_size = 100):
    if t.ndim != 1:
        t = np.argmax(t, axis = 1)
    
    acc = 0.0

    for i in range(int(x.shape[0] / batch_size)):
        tx = x[i*batch_size:(i+1)*batch_size]
        tt = t[i*batch_size:(i+1)*batch_size]
        y = predict(network, tx)
        y = np.argmax(y, axis = 1)
        acc += np.sum(y == tt)
    
    return acc / x.shape[0]

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, one_hot_label=False)
x_test = x_test[:1000]
t_test = t_test[:1000]


with open("sample_weight.pkl", "rb") as f:
    network = pickle.load(f)


print("Accuracy: "+ str(accuracy(x_test, t_test, network)))
```
