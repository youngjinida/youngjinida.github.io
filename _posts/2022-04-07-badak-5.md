---
title:  "학습 관련 기술들 - 2" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-04-11
last_modified_at: 2022-04-11
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 5. 학습 관련 기술 - 2

전 글에서는 최적값을 탐색하는 Optimizer, Weight Initialization, Batch Normalization 등에 대해서 알아보았다. 이번 글에서는 전 글에이어서 학습 관련 기술중에서도 오버피팅의 대응책인 가중치 감쇠, 드롭아웃 등에대해서 알아볼 예정이다.

## 5.1 오버피팅 

기계학습에서는 오버피팅이 문제가 되는 일이 많다. 여기서 오버피팅이란 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태를 말하고 기계 학습은 범용 성능을 지향한다. 훈련 데이터에는 포함되지 않는 아직 보지 못한 데이터가 주어져도 바르게 식별해내는 모델이 바람직하며 복잡하고 표현력이 높은 모델을 만들수는 있지만 그만큼 오버피팅을 억제하는 기술이 중요해지게된다.

주로 오버피팅은 매개변수가 많고 표현력이 높은 모델을 사용할 때 또는 훈련 데이터가 적을 떄 발생한다. 보통 오버피팅이 발생할 때의 정확도 그래프는 다음과 같이 나타난다.

![img5-1](/assets/images/badak/fig%206-20.png)

훈련 데이터를 사용하여 측정한 정확도는 약 100epoch에서 부터 100%의 정확도를 보이지만 그러나 시험 데이터에 대해서는 낮은 정확도를 보이며 이처럼 정확도가 크게 벌어지는 것은 훈련 데이터에만 적응해버린 결과이며 훈련 때 사용하지 않은 범용(시험)데이터에는 제대로 대응하지 못하는 것을 이 그래프에서 확인할 수 있다.

이제 오버피팅을 억제하기 위해서 사용하는 방법을 소개한다.

## 5.2 가중치 감쇠

오버피팅 억제용으로 예로부터 많이 이용해온 방법 중 하나로는 <u>가중치 감쇠</u>가 있다. 이는 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 페널티를 부과하여 오버피팅을 억제하는 방법이다. 큰 가중치에 대해서 페널티를 부과하는 이유는 오버피팅이 발생할 때는 가중치 매개변수의 값이 너무 커서 발생하는 경우가 많기 때문이다.

이제 가중치 감쇠를 어떻게 설게하느냐인데 보통 사용하는 방법으로는 우리가 신경망을 학습시키는 척도로써 사용하는 손실 함수에 가중치의 2차 Norm을 더해줌으로써 가중치가 커지는 것을 억제하게 된다. 가중치를
$W$
라고한다면 가중치 감쇠항은
$\frac{1}{2}\lambda W^2$
이며 여기서
$\lambda$
는 정규화의 세기를 조절하는 하이퍼파라미터이다. 람다의 크기가 커질수록 큰 가중치에 대한 페널티가 커지게된다. 그리고 가중치의 기울기를 구하는 계산에서는 그동안의 오차역전파법에 따른 결과에 정규화 항을 미분한
$\lambda W$
를 더한다.

실험을 진행하는 코드는 다음과 같다. 여기서 사용하는 MultiLayerNet은 전 포스팅의 MultiLayerNet.py를 불러와서 진행하면 된다. 오버피팅을 의도적으로 일으키기 위해서 은닉층을 깊게하고 훈련 데이터를 줄여서 실험을 진행하였다.

```python
import numpy as np
import matplotlib.pyplot as plt

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

x_train = x_train[:300]
t_train = t_train[:300]

weight_decay_lambda = 0.1


network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10, weight_decay_lambda=weight_decay_lambda)

optimizer = SGD(lr=0.01)

max_epochs = 201
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
epoch_cnt = 0

for i in range(1000000000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))

        epoch_cnt += 1
        if epoch_cnt >= max_epochs:
            break

markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

아래 결과를 보면 훈련 데이터에 대한 정확도와 시험 데이터에 대한 정확도에는 여전히 차이가 있지만 가중치 감소를 이용하지않은 5.1 오버피팅의 예시와 비교하면 그 차이가 줄어든 것을 확인할 수 잇다. 다시말해 오버피팅이 억제되었다는 소리이다. 또한 훈련데이터의 정확도가 100%에 도달하지 못한 점도 주목해야할 부분이다.

## 5.3 드롭아웃

앞에서 본 가중치 감쇠는 어려운 구현이 필요하지 않고 어느 정도 지나친 학습을 억제할 수 있다. 하지만 신경망 모델이 복잡해질수록 가중치 감쇠만으로는 대응하기 어려워진다. 이럴 때는 흔히 드롭아웃이라는 기법을 이용한다.

드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법이며 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제하고 삭제된 뉴런은 아래 그림과 같이 신호를 전달하지 않게 된다. 훈련 때는 데이터를 흘릴 때 마다 삭제할 뉴런을 무작위로 선택하고 시험 때는 모든 뉴런에 신호를 전달한다. 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제하지 않은 비율을 곱하여 출력한다. 구현은 아래와 같다.

```python
class Dropout:
    def __init__(self, dropout_ratio = 0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
    
    def forward(self, x, train_flg = True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1. - self.dropout_ratio)
    
    def backward(self, dout):
        return dout * self.mask
```

또한 이 드롭아웃을 앞의 실험과 마찬가지로 하였을 때 결과는 아래와 같다.

![img5-2](/assets/images/badak/fig%206-23.png)

그림과 같이 드롭아웃을 적용하니 훈련 데이터와 시험데이터에 대한 정확도 차이가 줄어든 것을 확인할 수 있고 훈련 데이터의 정확도도 100%에 도달하지 않게 되었다.

## 5.4 적절한 하이퍼파라미터 값 찾기

신경망에는 하이퍼파라미터가 다수 등장한다. 여기서 하이퍼파라미터는 각 층의 뉴런 수, 배치 사이즈, 학습률과 가중치 감쇠등이 있다. 이러한 하이퍼파라미터의 값을 적절히 설정하여야 올바른 학습이 될수 있다. 하지만 이러한 하이퍼파라미터 값을 찾는데에는 많은 시간이 걸린다. 이번엔 하이퍼파라미터의 값을 효율적으로 탐색하는 방법을 알아보자.

지금까지는 데이터셋을 훈련 데이터와 시험 데이터라는 두 가지로 분리해 이용했다. 훈련 데이터로는 학습을 하고, 시험데이터로는 범용 성능을 평가하였다. 그렇게 하여 훈련 데이터에만 지나치게 적응되어 있지않은지, 그리고 범용성능은 어느 정도인지를 평가할 수 있어다.

앞으로 하이퍼파라미터를 다양한 값으로 설정하고 검증해야하는데 여기서 주의할 점은 하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용해선 안된다는 것이다. 같은 성능 평가지만 하이퍼파라미터가 대상일 때는 시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼 파라미터의 값이 시험데이터에 오버피팅되기 때문이다. 그래서 하이퍼파라미터를 조정할 때는 하이퍼파라미터 전용 확인 데이터인 <u>검증 데이터</u>가 필요하게된다.

데이터셋에 따라서는 훈련, 검증, 시험 데이터를 미리 분리해둔 것도 있지만 MNIST의 경우에는 훈련 데이터와 시험 데이터로만 분리해두었다. 이런 경우에는 사용자가 직접 데이터를 분리하여야 한다. MNIST 데이터셋에서 검증 데이터를 얻는 방법은 훈련 데이터 중 20%를 검증 데이터로 먼저 분리하는 것이다. 코드로는 다음과 같다.

```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)

x_train, t_train = shuffle_dataset(x_train, t_train)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]
```

이제 하이퍼파라미터의 최적화를 해보자. 하이퍼파라미터를 최적화할 때의 핵심은 하이퍼파라미터의 최적 값이 존재하는 범위를 조금씩 줄여간다는 것이다. 범위를 조금씩 줄이기 위해서는 우선 대략적인 범위를 설정하고 그 범위에서 랜덤으로 하이퍼파라미터 값을 샘플링한 후 그 값으로 정확도를 평가한다. 정확도를 잘 살피면서 이 작업을 여러 번 반복하며 하이퍼파라미터의 최적 값의 범위를 좁혀나가는 것이다. 하이퍼파라미터의 범위는 대략적으로 지정하는 것이 효과적이며 실제로도 0.001에서 1000 사이와 같이 10의 거듭제곱 단위로 범위를 지정하고 이를 로그스케일로 지정한다고 한다. 또한 하이퍼파라미터를 최적화할 때는 오랜시간이 걸리며 그래서 학습을 위한 에폭을 작게 하여, 1회 평가에 걸리는 시간을 단축하는 것이 효과적이다.

이제 이론을 기반으로 코드를 작성하면 다음과 같다. 여기서는 가중치 감쇠 계수의 범위를 
$10^-8 - 10^-4$
, 학습률의 범위를 
$10^-6 ~ 10^-3$
로 설정한다.

```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.util import shuffle_dataset
from common.trainer import Trainer

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

x_train = x_train[:500]
t_train = t_train[:500]

validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)
x_train, t_train = shuffle_dataset(x_train, t_train)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]


def __train(lr, weight_decay, epocs=50):
    network = MultiLayerNet(input_size=784,
                            hidden_size_list=[100, 100, 100, 100, 100, 100],
                            output_size=10, weight_decay_lambda=weight_decay)
    trainer = Trainer(network, x_train, t_train, x_val, t_val,
                      epochs=epocs, mini_batch_size=100,
                      optimizer='sgd',
                      optimizer_param={'lr': lr}, verbose=False)
    trainer.train()

    return trainer.test_acc_list, trainer.train_acc_list

optimization_trial = 100
results_val = {}
results_train = {}
for _ in range(optimization_trial):
    weight_decay = 10 ** np.random.uniform(-8, -4)
    lr = 10 ** np.random.uniform(-6, -2)

    val_acc_list, train_acc_list = __train(lr, weight_decay)
    print("val acc:" + str(val_acc_list[-1]) + " | lr:" + str(lr) + ", weight decay:" + str(weight_decay))
    key = "lr:" + str(lr) + ", weight decay:" + str(weight_decay)
    results_val[key] = val_acc_list
    results_train[key] = train_acc_list

print("=========== Hyper-Parameter Optimization Result ===========")
graph_draw_num = 20
col_num = 5
row_num = int(np.ceil(graph_draw_num / col_num))
i = 0

for key, val_acc_list in sorted(results_val.items(), key=lambda x: x[1][-1], reverse=True):
    print("Best-" + str(i+1) + "(val acc:" + str(val_acc_list[-1]) + ") | " + key)

    plt.subplot(row_num, col_num, i+1)
    plt.title("Best-" + str(i+1))
    plt.ylim(0.0, 1.0)
    if i % 5:
        plt.yticks([])
    plt.xticks([])
    x = np.arange(len(val_acc_list))
    plt.plot(x, val_acc_list)
    plt.plot(x, results_train[key], "--")
    i += 1

    if i >= graph_draw_num:
        break

plt.show()

```

또한 코드를 실행시킨 결과는 다음과 같다. 이 결과를 보았을 때 학습이 잘진행 되었을 때의 학습률은 0.001 ~ 0.01이며 가중치 감쇠 계수는 
10^-8 ~ 10^-6 정도임을 알 수 있다. 또한 이 범위내에서 다시 범위를 좁혀가는 작업을 반복한 뒤 적절한 값이 위치한 범위를 좁혀가다 특정 단계에서 최종 하이퍼파라미터 값을 하나 선택한다.

![img5-3](/assets/images/badak/Figure_3.png)
```bash
=========== Hyper-Parameter Optimization Result ===========
Best-1(val acc:0.82) | lr:0.006183060866214522, weight decay:4.6290174418472983e-08
Best-2(val acc:0.81) | lr:0.008340891439682254, weight decay:1.3547223334933814e-06
Best-3(val acc:0.77) | lr:0.006381945113201277, weight decay:3.422403255859695e-07
Best-4(val acc:0.73) | lr:0.006606157468896251, weight decay:1.7272268233372106e-08
Best-5(val acc:0.64) | lr:0.004255397879121311, weight decay:7.557466018392538e-05
Best-6(val acc:0.64) | lr:0.006481420712269814, weight decay:8.373958937418399e-07
Best-7(val acc:0.63) | lr:0.003759798142134359, weight decay:4.224650113669884e-07
Best-8(val acc:0.63) | lr:0.0060181586969884175, weight decay:4.1334366466900375e-07
Best-9(val acc:0.57) | lr:0.00390594625351148, weight decay:1.979506748519445e-07
Best-10(val acc:0.53) | lr:0.0039330502418528715, weight decay:1.6514326877598307e-05
Best-11(val acc:0.47) | lr:0.004923238800340969, weight decay:5.118976811603265e-06
Best-12(val acc:0.46) | lr:0.0035227261948620144, weight decay:2.4808806473656428e-05
Best-13(val acc:0.45) | lr:0.0025322198151598824, weight decay:1.1105612038762738e-06
Best-14(val acc:0.33) | lr:0.0017268834499571566, weight decay:2.8574024813290752e-06
Best-15(val acc:0.33) | lr:0.002378418491316931, weight decay:2.0813369287163807e-08
Best-16(val acc:0.32) | lr:0.0009463022676072101, weight decay:5.42728145927435e-08
Best-17(val acc:0.29) | lr:0.001754514685110173, weight decay:4.128208951359877e-06
Best-18(val acc:0.25) | lr:0.00012562555581510063, weight decay:1.714076022420647e-08
Best-19(val acc:0.25) | lr:0.0014563075522347904, weight decay:3.1336415255933275e-08
Best-20(val acc:0.23) | lr:0.001555709466710158, weight decay:1.48049041431014e-05
```