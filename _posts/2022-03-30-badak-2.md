---
title:  "신경망 학습 및 구현" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-03-30
last_modified_at: 2022-03-30
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 2. 신경망 학습
## 2.1 신경망을 학습한다는 것은

여기서 말하는 학습이란 훈련 데이터로부터 가중치의 최적값을 자동으로 얻는 것을 말한다. 실제 사용하게되는 신경망의 경우 가중치의 매개변수가 수천, 수만개를 넘게되는데 이 매개변수들을 사람이 설정한다는 것은 불가능한 일에 가깝다. 또한 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔으며 이러한 신경망을 학습하기 위해서는 <u>손실 함수</u>라는 것을 이용하게 되는데 이 손실 함수의 결괏값을 작게만드는 가중치를 찾는 것이 학습의 최종 목표가 된다. 

## 2.2 이미지의 특징

아래 이미지에서 '5' 라는 숫자를 인식하는 프로그램을 구현한다고 하였을 때 비교적 간단하게 보일지 모르겠지만 실제 프로그램을 직접 고안하여 설계하는 것은 어려운 문제이다.

![img4-1](/assets/images/badak/fig%204-1.png)

사람이라면 어렵지않게 인식할 수 있음에도 불구하고 여러가지 필체에서 '5'를 특정짓는 규칙을 찾는 것은 어려운 일이기 때문이다. 그래서 나온 방법중 하나로 이미지에서 <u>특징</u>을 뽑아내어 그 특징의 패턴을 기계학습을 통하여 학습하는 방법이있다. 여기서 말하는 특징이란 입력 데이터에서 본질적인 데이터를 추출하는 것을 말하며 컴퓨터 비전 분야에서는 HOG, SIFT, SURF 등을 특징과 색상 히스토그램 등을 이용하여 이미지 데이터를 벡터로 변환하고 변환된 벡터를 통하여 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습을 한다.

위와 같은 기계학습에서는 모아진 데이터로 부터 규칙을 찾아내는 역할은 기계가 담당하게 되지만 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계하는 것임에 주의 해야한다. 문제에 맞는 적합한 특징을 쓰지 않게되면 좋은 결과를 얻지 못하기 때문이다.

## 2.3 훈련 데이터와 시험 데이터

신경망을 학습시키기에 앞서 기계학습에서 데이터를 취급할 때에는 주의를 하여야한다. 보통의 기계학습 문제에서는 데이터를 훈련 데이터와 시험 데이터로 나누어 실험을 수행한다. 훈련 데이터를 통하여 최적의 가중치를 찾고 그 다음 시험 데이터를 사용하여 훈련한 모델을 평가하는 것이다. 그 이유는 우리가 원하는 것은 범용적으로 사용할 수 있는 모델이기 때문이다. 여기서 범용 능력이란 아직 보지 못한 데이터에 대해 문제를 올바르게 풀어낼 수 있는 능력을 말하며 이러한 능력을 올바르게 평가하기 위해서 훈련 데이터와 시험 데이터를 분리하는 것이다.

## 2.4 손실 함수란?

손실 함수란 신경망 학습에서의 현재 상태를 하나의 지표로 표현한다. 그리고 그 지표를 가장 좋게 만들어 주는 가중치 매개변수의 값을 탐색하게 된다. 이 손길 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 오차제곱합(SSE)와 교차 엔트로피 오차(CEE)를 사용한다.

### 오차 제곱합

$$
E = \frac{1}{2}{\sum_{k} (y_k - t_k)^2}
$$

여기서 $y_k$는 신경망의 출력을 뜻하며 $t_k$ 정답 레이블을 뜻한다. MNSIT의 경우 이미지가 7을 나타낼 때 레이블 링이 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]으로 정답에 대한 인덱스의 원소만 1이고 나머지는 0으로 표현 되어있다. 이런 식의 표현을 원-핫 인코딩이라고한다.

### 교차 엔트로피 오차

$$
E = -{\sum_{k} t_k log(y_k)}
$$

여기서 로그는 밑이 e인 자연로그를 뜻하며 위의 오차 제곱합과 같이 $y_k$는 신경망의 출력을 뜻하며 $t_k$ 정답 레이블을 뜻한다. 교차 제곱합의 경우 기회가 된다면 나중에 이론 부분을 정리하여 포스팅할 예정이다.

***loss.py***
```python
def sum_squares_error(y, t):
	return 0.5 * np.sum((y - t)**2)

def cross_entropy_error(y, t):
	delta = 1e-7
	# y의 값이 0이 되었을 때 np.log의 값이 -inf가 되므로 delta를 더하여 -inf가 되지 않도록 함
	return -np.sum(t*np.log(y+delta))
```

두 손실 함수의 예를 한번 살펴보자

***loss_func_example.py***
```python
t_1 = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
y_1 = np.array([0.1, 0.05, 0.2, 0.4, 0.05, 0.05, 0.05, 0, 0, 0])

t_1 = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
y_1 = np.array([0.1, 0.05, 0.2, 0.05, 0.4, 0.05, 0.05, 0, 0, 0])

print("sse_1: " + str(sum_squares_error(y_1, t_1))
print("cee_1: " + str(cross_entropy_error(y_1, t_1))

print("sse_2: " + str(sum_squares_error(y_2, t_2))
print("cee_2: " + str(cross_entropy_error(y_2, t_2))
```

```bash
sse_1: 0.21000000000000002
cee_1: 0.9162904818741863
sse_2: 0.56
cee_2: 2.9957302735559908
```

두 함수의 결과 값 모두 실제 답과 예측한 값이 다를 때 오류의 값이 더 큰 것을 확인할 수 있다. 즉, 결과가 더 작은 첫번째 추정이 정답일 가능성이 높다고 판단한 것이므로 손실함수로써 올바른 역할을 한다고 생각할 수 있다.


## 2.5 미니배치 학습

기계학습 문제는 훈련 데이터를 사용하여 학습하게 된다. 더 궤적으로 말하자면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. 이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 하며 즉, 훈련 데이터가 100개 있을 때 그로부터 계산한 100개의 손실 함수의 값을 지표로 삼게된다.

훈련 데이터 모두에 대한 손실 함수의 합을 구하는 것은 아래와 같다.

$$
E = -\frac{1}{N}{\sum_{n}\sum_{k} t_{nk} log(y_{nk})}
$$

이는 위에서 보았던 손실함수인 교차 엔트로피 오차를 배치 학습을 시킬 때의 식을 의미하며 간단히 설명하자면 총 N개의 데이터에 대하여 손실함수를 구한뒤 N으로 나누어 정규화한 것이다. 하지만 이 때 총 데이터의 개수가 많으면 많을 수록 손실 함수의 값을 구하는데 오래 걸리게 되므로 데이터에 일부를 추려 전체의 '근사치'로 이용할 수 있다. 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행하는데 이 일부를 <u>미니배치</u>라고한다.


## 2.6 미분

### 수치미분
중앙 차분을 통하여 수치 미분을 위한 식은 아래와 같다.

$$

 \frac{df}{dx} = \frac{f(x+h) - f(x-h)}{2h}

$$

여기서 h는 0에 근접한 값으로 표현하며 파이썬을 통하여 구현하면 아래와 같다.

***numerical_diff.py***
```python
def numerical_diff(f, x):
    h = 1e-4 
    return (f(x+h) - f(x-h)) / (2*h)
```

이 수치미분을 통하여 아래의 간단한 함수를 미분해보자

$$
y = 0.01x^2 + 0.1x
$$

***numerical_diff_test.py***
```python
def func(x):
    return 0.01*(x**2) + 0.1*x

print("result_1: " + str(numerical_diff(func, 5)))
print("result_2: " + str(numerical_diff(func, 10)))
```

```bash
result_1: 0.1999999999990898
result_2: 0.2999999999986347
```

위 함수를 미분하여 5와 10에서의 기울기를 구하면 0.2, 0.3이며 앞의 수치 미분과의 결과를 비교하였을 때 오차가 매우 작은 것을 알 수 있다.

### 편미분

편미분이란 다변수 함수의 특정변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것을 말하며 아래의 식을 파이썬 코드를 통하여 편미분을 구현해보자

$$
f(x_0, x_1) = x_0^2 + x_1^2
$$

$$
x_0 = 3, x_1 = 4
$$

***partial_derivative.py***
```python
def func_1(x):
    return x[0]**2 + x[1]**2

def numerical_diff(f, x):
    h = 1e-4
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp = x[idx]
        x[idx] = tmp + h
        xh_1 = f(x)
        x[idx] = tmp - h
        xh_2 = f(x)
        
        grad[idx] = (xh_1 - xh_2) / (2*h)
        
        x[idx] = tmp

    return grad

print(numerical_diff(func_1, np.array([3., 4.])))
```

```bash
[6. 8.]
```

위 구현을 통하여 얻은 값인 (6, 8)에 대한 의미를 알기 위해서 위 함수에서 각 점의 편미분 값을 그림으로 표현해보겠다.

![img4-2](/assets/images/badak/fig%204-9.png)

위 그림을 보았을 때 각 편미분의 값인 기울기는 함수의 가장 낮은 장소를 가르키는 것을 알 수 있다. 또한 가장 낮은 곳에서 멀어질수록 벡터의 크기가 작아지는 것 또한 알 수 있다. 하지만 하나 유의해야할 점으로 기울기는 각 지점에서 낮아지는 방향을 가르키며 정확히 말하자면 기울기가 가르키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.

## 2.7 경사하강법

기계학습 문제의 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수를 학습 시에 찾아야 하며 여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수의 값을 말한다. 그러나 일반적인 문제의 손실함수는 매우 복잡하며 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지 짐작하기란 어려운 문제이며 이런 상황에서 기울기를 잘 이용하여 함수의 최솟값을 찾으려는 것이 경사법이다.

그러나 기울기가 가르키는 곳이 정말 함수의 최솟값이 있는지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가르키는 방향에 최솟값이 없는 경우가 대부분이다. 함수가 극솟값, 최소값 또한 안장점이 되는 장소에서는 기울기가 0이다. 극솟값은 국소적인 최솟 값 안장점은 어느방향에서 보았을때는 극솟값이 되는 점이며 다른 방향에서 보았을 때는 극대값인 점을 말한다. 아래의 그림을 보면 이해를 하기쉽다.

<p align="center">
	<img src = "/assets/images/badak/ajangjum.jpg" />
</p>

위 설명과 같이 기울어진 방향이 반드시 최소값을 가르키는 것은 아니나 그 방향으로 진행하였을 때 함수의 값을 줄일 수 있으며 최소값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 결정해야한다. 그렇게 나온 경사하강법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한 뒤 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 기울어진 방향으로 나아가기를 반복하여 함수의 값을 점차 줄여나가는 것이 <u>경사하강법</u>이다.

이 경사하강법을 수식으로 나타내면 다음과 같다.

$$
x_0 = x_0 - \lambda \frac{df}{dx_0}
$$


$$
x_1 = x_1 - \lambda \frac{df}{dx_1}
$$

여기서 $$\lambda$$ 는 갱신하는 양을 나타내며 신경망 학습에서는 학습률이라고 한다. 한번의 학습으로 매개변수를 얼마나 갱신하느냐를 정하는 것이 학습률이다. 학습률의 값은 0.01, 0.001과 같은 특정 값으로 미리 정해두어야 하며 너무 크거나 작은 값으로 설정하게 되면 최적의 값을 찾기 어려워진다. 신경망 학습에서는 보통 이 학습률의 값을 변경하면서 올바르게 학습하고 있는지를 확인하며 진행한다.

경사하강법의 구현은 아래와 같다.

***gradient_decent.py***
```python

def gradient_decent(f, init_x, lr=0.1, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_diff(f, x)
        x -= lr*grad
    return x

print(gradient_decent(func_1, np.array([-3., 4.]), lr = 0.01, step_num=100))
```

```bash
[-6.11110793e-10  8.14814391e-10]
```

위에서 작성한 함수인 $$f(x_0, x_1) = x_0^2 + x_1^2$$ 의 최소값은 (0, 0)이며 최종 결과인 (-6.11110793e-10  8.14814391e-10)에 아주 가깝다고 볼 수 있으며 거의 정확한 결과를 얻은 것으로 볼 수 있다.

## 2.8 신경망 구현

신경망 학습에서도 기울기를 구해야한다. 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기를 말하며 예를들어 형상이 2x3, 가중치가 $$W$$, 손실함수가 $$L$$인 신경망을 만들었을 때. 이경우는 경사는 $$\frac{dL}{dW}$$ 로 나타낼 수 있다. 수식으로는 아래와 같다.

$$
W=
\begin{pmatrix}
  w_{11} & w_{12} & w_{13}\\
  w_{21} & w_{22} & w_{23} \\
 \end{pmatrix}
$$

$$
\frac{dL}{dW} = 
\begin{pmatrix}
  \frac{dL}{dw_{11}} & \frac{dL}{dw_{12}} & \frac{dL}{dw_{13}}\\
  \frac{dL}{dw_{21}} & \frac{dL}{dw_{22}} & \frac{dL}{dw_{23}} \\
 \end{pmatrix}
$$

위 $$\frac{dL}{dW}$$의 각 원소는 W의 각각의 원소에 관한 편미분이며 예를들어 1행 1번째 원소인 $$\frac{dL}{dw_{11}}$$ 는 $$w_{11}$$을 아주 조금 변경했을 때 손실함수 L이 얼마나 변화하느냐를 나타낸다.

위의 미분 내용을 토대로 $$dW$$ 를 구하는 코드를 구현해보자

***simple_net.py***
```python
import numpy as np

def softmax(x):
    return np.exp(x)/np.sum(np.exp(x))

def cross_entropy(y, t):
    if y.ndim == 1:
        y = y.reshape(1, y.size)
        t = t.reshape(1, t.size)
    
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + (1e-7))) / batch_size

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3)

    def predict(self, x):
        return x@self.W

    def loss(self, x, t):
        z=self.predict(x)
        y=softmax(z)
        loss=cross_entropy(y, t)

        return loss

def numerical_gradient(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)

        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2) / (2*h)

        x[idx] = tmp_val
        it.iternext()

    return grad

def f(W):
    return net.loss(x, t)


net = simpleNet()

x = np.array([0.6, 0.9])
p = net.predict(x)

t = np.array([0, 0, 1])
net.loss(x, t)

dW = numerical_gradient(f, net.W)
print(dW)
```

```bash
[[ 0.16483614  0.30002738 -0.46486352]
 [ 0.24725421  0.45004106 -0.69729528]]
```

결과는 W의 크기와 똑같은 2x3의 크기인 것을 알 수 있고 결괏값을 확인하였을 때 $$w_{11}$$을 1만큼 늘리면 0.1648만큼 손실함수의 값이 증가하고 $$w_{23}$$을 1만큼 증가시킬 경우 0.6972만큼 감소함을 알 수 있다. 그러므로 손실함수의 값을 줄인다는 관점에서 볼때는 $$w_{11}$$은 음의 방향으로 갱신하고 $$w_{23}$$은 양의 방향으로 갱신시켜야 함을 알 수 있다.

마지막으로 학습 학습 알고리즘을 구현하면 아래와 같다. 입력층의 크기는 MNIST 손글씨 이미지의 크기인 28*28을 flatten하여 784이며 출력층은 10가지의 분류를 하기위해서 10으로 설정하였으며 은닉층의 크기는 100으로 설정하였다. 또한 학습률은 0.1 미니배치의 크기는 100으로 설정하여 구현을 진행하였다.

***TwoLayerNet.py***
```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        self.param = {}
        self.param['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std
        self.param['b1'] = np.zeros(hidden_size)
        self.param['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std
        self.param['b2'] = np.zeros(output_size)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        return np.exp(-x)/np.sum(np.exp(-x))

    def cross_entropy(self, y, t):
        if y.ndim == 1:
            y = y.reshape(1, y.size)
            t = t.reshape(1, t.size)
        
        batch_size = y.shape[0]
        return -np.sum(t * np.log(y + (1e-7)))/batch_size

    def predict(self, x):
        W1, W2 = self.param['W1'], self.param['W2']
        b1, b2 = self.param['b1'], self.param['b2']
        a1 = x @ W1 + b1
        z1 = self.sigmoid(a1)
        a2 = z1 @ W2 + b2
        y = self.softmax(a2)
        return y
    
    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis = 1)
        t = np.argmax(t, axis = 1)

        accuracy = np.sum(y == t) / x.shape[0]
        return accuracy

    def numerical_gradient(self, f, x):
        h = 1e-4
        grad = np.zeros_like(x)

        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
        while not it.finished:
            idx = it.multi_index
            tmp_val = x[idx]
            x[idx] = float(tmp_val) + h
            fxh1 = f(x)

            x[idx] = tmp_val - h
            fxh2 = f(x)
            grad[idx] = (fxh1 - fxh2) / (2*h)

            x[idx] = tmp_val
            it.iternext()

        return grad


    def gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = self.numerical_gradient(loss_W, self.param['W1'])
        grads['b1'] = self.numerical_gradient(loss_W, self.param['b1'])
        grads['W2'] = self.numerical_gradient(loss_W, self.param['W2'])
        grads['b2'] = self.numerical_gradient(loss_W, self.param['b2'])
        return grads

# Train start
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []

iter_num = 10000
train_size = x_train.shape[0]
batch_size = 100
lr = 0.1
network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)
iter_per_epoch = max(train_size/batch_size, 1)

train_acc_list = []
test_acc_list = []

for i in range(iter_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.param[key] -= lr * grad[key]
    
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, x_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)


```
