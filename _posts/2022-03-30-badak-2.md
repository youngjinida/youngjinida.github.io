---
title:  "신경망 학습 및 구현" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-03-30
last_modified_at: 2022-03-30
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 2. 신경망 학습
## 2.1 신경망을 학습한다는 것은

여기서 말하는 학습이란 훈련 데이터로부터 가중치의 최적값을 자동으로 얻는 것을 말한다. 실제 사용하게되는 신경망의 경우 가중치의 매개변수가 수천, 수만개를 넘게되는데 이 매개변수들을 사람이 설정한다는 것은 불가능한 일에 가깝다. 또한 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔으며 이러한 신경망을 학습하기 위해서는 <u>손실 함수</u>라는 것을 이용하게 되는데 이 손실 함수의 결괏값을 작게만드는 가중치를 찾는 것이 학습의 최종 목표가 된다. 

## 2.2 이미지의 특징

아래 이미지에서 '5' 라는 숫자를 인식하는 프로그램을 구현한다고 하였을 때 비교적 간단하게 보일지 모르겠지만 실제 프로그램을 직접 고안하여 설계하는 것은 어려운 문제이다.

![img4-1](/assets/images/badak/fig%204-1.png)

사람이라면 어렵지않게 인식할 수 있음에도 불구하고 여러가지 필체에서 '5'를 특정짓는 규칙을 찾는 것은 어려운 일이기 때문이다. 그래서 나온 방법중 하나로 이미지에서 <u>특징</u>을 뽑아내어 그 특징의 패턴을 기계학습을 통하여 학습하는 방법이있다. 여기서 말하는 특징이란 입력 데이터에서 본질적인 데이터를 추출하는 것을 말하며 컴퓨터 비전 분야에서는 HOG, SIFT, SURF 등을 특징과 색상 히스토그램 등을 이용하여 이미지 데이터를 벡터로 변환하고 변환된 벡터를 통하여 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습을 한다.

위와 같은 기계학습에서는 모아진 데이터로 부터 규칙을 찾아내는 역할은 기계가 담당하게 되지만 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계하는 것임에 주의 해야한다. 문제에 맞는 적합한 특징을 쓰지 않게되면 좋은 결과를 얻지 못하기 때문이다.

## 2.3 훈련 데이터와 시험 데이터

신경망을 학습시키기에 앞서 기계학습에서 데이터를 취급할 때에는 주의를 하여야한다. 보통의 기계학습 문제에서는 데이터를 훈련 데이터와 시험 데이터로 나누어 실험을 수행한다. 훈련 데이터를 통하여 최적의 가중치를 찾고 그 다음 시험 데이터를 사용하여 훈련한 모델을 평가하는 것이다. 그 이유는 우리가 원하는 것은 범용적으로 사용할 수 있는 모델이기 때문이다. 여기서 범용 능력이란 아직 보지 못한 데이터에 대해 문제를 올바르게 풀어낼 수 있는 능력을 말하며 이러한 능력을 올바르게 평가하기 위해서 훈련 데이터와 시험 데이터를 분리하는 것이다.

## 2.4 손실 함수란?

손실 함수란 신경망 학습에서의 현재 상태를 하나의 지표로 표현한다. 그리고 그 지표를 가장 좋게 만들어 주는 가중치 매개변수의 값을 탐색하게 된다. 이 손길 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 오차제곱합(SSE)와 교차 엔트로피 오차(CEE)를 사용한다.

### 오차 제곱합
$$
E = \frac{1}{2}{\sum_{k} (y_k - t_k)^2}
$$

여기서 $y_k$는 신경망의 출력을 뜻하며 $t_k$ 정답 레이블을 뜻한다. MNSIT의 경우 이미지가 7을 나타낼 때 레이블 링이 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]으로 정답에 대한 인덱스의 원소만 1이고 나머지는 0으로 표현 되어있다. 이런 식의 표현을 원-핫 인코딩이라고한다.

### 교차 제곱합
$$
E = -{\sum_{k} t_k log(y_k)}
$$

여기서 로그는 밑이 e인 자연로그를 뜻하며 위의 오차 제곱합과 같이 $y_k$는 신경망의 출력을 뜻하며 $t_k$ 정답 레이블을 뜻한다. 교차 제곱합의 경우 기회가 된다면 나중에 이론 부분을 정리하여 포스팅할 예정이다.

***loss.py***
```python
def sum_squares_error(y, t):
	return 0.5 * np.sum((y - t)**2)

def cross_entropy_error(y, t):
	delta = 1e-7
	# y의 값이 0이 되었을 때 np.log의 값이 -inf가 되므로 delta를 더하여 -inf가 되지 않도록 함
	return -np.sum(t*np.log(y+delta))
```

### 신경망의 예
<center>

![img3-1](../img/badak/fig%203-1.png)

</center>

위 신경망은 모두 3층으로 구성이되지만 가중치를 갖는 층은 2개뿐이기 때문에 <u>2층 신경망</u>이라고 한다.

## 1.4 활성화 함수
활성화 함수란 이름과 같이 입력신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. 가중치가 달린 입력신호와 편항의 총합을 계산하고 이를 a라고 한다. 그리고 a를 활성화 함수 h를 통하여 y를 출력하는 흐름이며 예는 아래와 같다.

<center>

$a = b + w_1x_1 + w_1x_1$

$y = h(a)$

![img3-2](../img/badak/fig%203-4.png)

</center>

또한 보통의 퍼셉트론에서는 계단 함수를 활성화 함수로 채용하는데 신경망에서는 계단 함수가 아닌 다른 활성 함수를 사용한다. 보통의 신경망에서 쉽게 사용가능한 활성함수로써 Sigmoid와 Relu를 소개한다.

### Step function

<center>

$h(x)=
\begin{cases}
1,\;if\;x>0\\
0,\;else
\end{cases}$

![img3-3](../img/badak/fig%203-6.png)

</center>

### Sigmoid

<center>

$
h(x) = \frac{1}{1 + exp(-x)}
$


![img3-4](../img/badak/fig%203-7.png)

</center>

### Relu

<center>

$h(x)=
\begin{cases}
x,\;if\;x>0\\
0,\;else
\end{cases}$

![img3-5](../img/badak/fig%203-9.png)

</center>

위 함수들을 보았을 때 공통적인 성질은 <u>비선형 함수</u>라는 것이다. 신경망에서는 활성화 함수로 선형 함수를 사용해서는 안되기 때문이다. 그 이유로는 선형 함수를 사용하여 은닉층을 늘려도 은닉층이 없는 네트워크와 똑같은 기능을 할 수 있다는 데에 있다. 간단한 예를들어 설명하자면 h(x) = cx 라는 활성 함수를 사용하고 3층짜리 신경망을 설계한다면 y = h(h(h(x))) 이며 값은 y = c^3x가 나올 것이고 y = ax, a=c^3 이와 같이 1층 짜리 신경망으로도 구현할 수 있다. 그러므로 층을 쌓으므로써 혜택을 얻고싶다면 활성함수로는 비선형 함수를 사용하여야 한다.

***Activation_Func.py***
```python
def step_function(x):
	y = x > 0
	return y.astype(np.int)

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

def Relu(x):
	return np.maximum(0, x)
```

## 1.5 출력층 설계
신경망은 분류와 회귀 모두에 이용할 수 있다. 다만 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라지게 되고 일반적으로는 회귀 문제에서는 항등 함수를, 분류에서는 소프트맥스 함수를 사용한다.

### 항등 함수
항등 함수는 입력을 그대로 출력한다. 입력과 출력이 항상 같다는 뜻의 항등이며 출력층에서 항등 함수를 사용한다면 입력 신호가 그대로 출력 신호가 된다.

### 소프트맥스

$$
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n} a_i}
$$

위의 수식과 같이 소프트맥스는 분자가 입력 신호의 지수함수, 분모는 모든 입력신호의 지수 함수의 함으로 구성이된다. 또한 소프트맥스 출력의 총합은 1이다. 출력의 총합이 1이 된다는 것은 소프트 맥스 함수의 중요한 성질이며 이러한 성질로 인해 소프트맥스 함수의 출력을 <u>확률</u>로써 해석할 수 있게된다.

***Out_Func.py***
```python
def identity_func(x):
	return x

def softmax(x):
	c = np.max(x)
	exp_x = np.exp(x - c)
	sum_exp_x = np.sum(exp_x)
	y = exp_x / sum_exp_x

	return y

```

출력층의 누련 수는 풀려는 문제에 맞게 적절히 정하면 된다. 분류에서는 분류하고싶은 클래스 수로 설정하는 것이 일반적이며 예를들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 아래의 그림과 같이 출력층의 개수를 10개로 정하면 된다.

<center>

![img3-6](../img/badak/fig%203.23.png)

</center>

## 1.6 데이터 셋
현재 공부하고 있는 책인 <u>밑바닥부터 시작하는 딥러닝</u>에서는 MNIST라는 손 글씨 데이터를 사용하고 있으며 앞으로의 딥러닝 관련 포스팅에서 사용할 데이터 셋 또한 MNIST를 사용할 예정이다. MNIST 데이터셋은 0부터 9까지의 숫자 이미지로 구성이되며 훈련 이미지 60,000장, 시험 이미지 10,000장으로 구성된다. 일반적으로 훈련 이미지들을 사용하여 모델을 학습하고 학습한 모델로 시험 이미지들을 얼마나 정확하게 분류하는지를 평가한다.

![img3-7](../img/badak/fig%203-24.png)

MNIST의 이미지 데이터는 28x28 크기의 1채널 이미지이며 각 픽셀은 0~255 값을 가진다. 또한 각 이미지에는 그 이미지가 실제 의미하는 숫자가 레이블링이 되어있다. 아래 링크는 MNIST 데이터 셋을 받을 수 있는 링크이다.

Link-1: http://yann.lecun.com/exdb/mnist/