---
title:  "오차역전파" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-04-01
last_modified_at: 2022-04-01
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 3. 오차역전파

앞에서 배운 수치미분으로 신경망을 학습시키는 것에대해서 이론을 살펴본 뒤 구현을 
해보았다. 이번 장에서는 시간은 오래 걸리지만 구현이 상대적으로 쉬운 수치미분이 아닌 고속으로 신경망을 학습시킬 수 있는 오차역전파에 대해서 살펴볼 예정이다.

## 3.1 오차역전파란?

오차역전파란 오차를 역으로 전파를 한다 라는데에서 지어진 이름이며 신경망의 가중치 매개변수를 고속으로 계산할 수 있는 방법이다. 이번 포스팅에서는 오차역전파를 계산 그래프를 통해 설명을 할 것이며 보통의 오차역전파를 이해하는데는 두 가지 방법이 있다. 하나는 수식을 통한 것이고 두 번째 방법은 계산 그래프를 통한 방법이다. 일반적으로는 수식을 통한 방법을 사용하지만 오차역전파법을 시각적으로 보며 이해를 하는 시간을 가질 것이다.


### 계산그래프

계산 그래프는 계산 과정을 그래프로 나타낸 것이다. 여기에서 그래프는 자료구조에서의 그래프를 말하며 복수의 노드와 에지로 표현된다. 계산 그래프는 계산 과정을 노드와 화살표로 표현하며 노드는 원으로 표기하고 원 안에 연산내용을 적는다. 또, 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다.

아래 문제의 풀이를 계산그래프로 나타내보자

 사과 1개의 가격은 100원이고 2개를 산다고 하였을 떄 가격은 얼마인가? 단 소비세 10%가 부과된다.

![img5-1](/assets/images/badak/fig%205-2.png)

다음 문제이다. 사과 1개의 가격은 100원이고 귤은 150원이다. 사과 2개, 귤 3개를 구매한다고 하였을 때 가격은 얼마인가? 단 소비세가 10%가 부과된다.

![img5-2](/assets/images/badak/fig%205-3.png)

위 두문제의 과정과 같이 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행한다.

1. 계산 그래프를 구성한다.
2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.

여기서 2. 의 과정을 순전파라고 하며 반대 방향으로 즉 오른쪽에서 왼쪽으로 전파하는 경우를 역전파라고 칭한다. 역전파는 이후 미분을 계산할 때 중요한 역할을 한다.

지금까지 계산 그래프를 사용하여 두 문제를 풀어보았다. 그래서 이 계산 그래프의 장점이 무엇인가 묻는다면 <u>국소적 계산</u>이라고 할 수 있다. 여기서 말하는 국소적 계산이란 아래 그림을 예를들어 설명하겠다.

![img5-3](/assets/images/badak/fig%205-4.png)

여러 식품을 구입하여 총 4,000원이 되었고 거기에 사과를 2개 구입함으로써 총 4,200원이 되었다. 여기서 국소적 계산이란 가령 사고와 그 외의 물품 값을 더하는 계산은 4,000원이 어떻게 계산되었느냐와는 상관없이, 단지 그 숫자를 더하기만 하면된다는 뜻이다. 각 노드는 자신과 관련한 계산외에는 신경쓸 것이 없다.

또한 계산 그래프의 다른 장점으로는 역전파를 통해 <u>미분</u>을 효율적으로 계산할 수 있다는 것이다. 이 또한 예를들어 설명하자면 위 사과 2개를 구매하는 문제에서 사과의 가격이 오르면 최종 금액에 어떤 영향을 미치는지에 대해 계산한다고 가정했을 때 이는 사과 가격에 대한 최종 금액의 미분 을 구하는 문제에 해당한다. 

![img5-4](/assets/images/badak/fig%205-5.png)

이 예에서는 역전파는 오른쪽에서 왼쪽으로 미분 값을 전달한다. 이 결과로 부터 사과 가격에 대한 지불금액의 미분 값은 2.2라고 할 수 있으며 사과가 1원이 올랐을 때 최종 금액은 2.2원이 오른다는 것이다.

### 연쇄법칙

그동안 해온 계산 그래프이 순전파는 계산 결과를 왼쪽에서 오른쪽으로 전달하였다. 이 순서는 평소의 하는 방식과 같아 자연스럽게 느껴졌을 것이다. 한편 역전파는 국소적인 미분을 순방향과는 반대인 오른쪽에서 왼쪽으로 전달한다. 또한 이 국소적 미분을 전달하는 원리는 연쇄법칙에 따른 것이며 따라서 연쇄 법칙을 설명하고 그것이 계산 그래프 상의 역전파와 같다는 사실을 설명한다. 아래는 $ y=f(x) $ 라는 계산의 역전파를 나타낸 것이다.

![img5-5](/assets/images/badak/fig%205-6.png)

위 그림과 같이 역전파의 계산 절차는 신호 E에 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다. 여기에서 말하는 국소적 미분은 순전파 때의 $ y=f(x) $ 계산의 미분을 구한다는 것이며 이는 x에 대한 y의 미분을 구한다는 뜻이다. 가령 $y=f(x)=x^2$라면 $\frac{dy}{dx}=2x$가 된다. 그리고 이 국소적 미분을 상류에서 전달된 E에 곱해 앞쪽 노드로 전달하는 것이다.

이것이 역전파의 계산 순서인데, 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심이다. 왜 이런 일이 가능한가는 연쇄법칙의 원리로 설명할 수 있다.

연쇄 버빅을 설명하려면 우선 합성 함수의 이야기부터 시작한다. 합성 함수란 여러 함수로 구성된 함수이다. 예를 들어 $z=(x+y)^2$의 식은 두개의 식으로 구성된다.

$$ z=t^2 $$

$$ t=x+y $$

합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다라는 것이 연쇄 법칙의 원리이며 이 또한 예로 설명하자면 $\frac{dz}{dx}$는 $\frac{dz}{dt}$ 와
$\frac{dt}{dx}$ 의 곱으로 설명할 수 있다는 것이다. 수식으로는 다음과 같이 쓸 수 있다.

$$ \frac{dz}{dx} = \frac{dz}{dt} \frac{dt}{dx} $$

위 식에서 $ dt $ 는 약분 가능하며 미분 $\frac{dz}{dx}$ 를 구해보자 

$$ \frac{dz}{dt} = 2t $$
$$ \frac{dt}{dx} = 1 $$

$$ \frac{dz}{dx} = 2t \cdot 1 = 2(x+y) \cdot 1$$

위와 같은 계산을 통하여 합성 함수의 미분을 하는 과정을 연쇄 법칙을 통하여 할 수 있게 되었으며 이 연쇄법칙 계산을 계산 그래프로 나타내면 아래와 같다.

![img5-6](/assets/images/badak/fig%205-8.png)

위 그림과 같이 계산 그래프의 역전파는 오른쪽에서 왼쪽으로 신호를 전달하며 역전파의 계산 절차에서는 노드로 들어온 입력신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다. 이 경우에서는 입력이 1이며 ($\frac{dz}{dz}$ 이므로) 국소적 미분인 $\frac{dz}{dt}$ 를 곱하고 다음 노드로 넘긴다.(순전파 시 입력이 t이고 출력이 z이므로 이 노드에서 미분은 $dz/dt$가 된다.) 또한 $\frac{dz}{dt} \cdot \frac{dz}{dz}$ 에 t에 대한 x의 미분인 $\frac{dt}{dx}$ 를 곱하면 $ \frac{dz}{dx} = \frac{dz}{dz} \frac{dz}{dt} \frac{dt}{dx} $ 가 되므로 즉, 역전파가 하는 일은 연쇄 법칙과 원리가 같다는 것을 알 수 있다.

## 3.2 단순한 계층 구현하기

모든 계층은 forward()와 backward()라는 공통의 메서드를 갖도록 구현한다. forward()는 순전파 backward()는 역전파를 처리한다.

그럼 먼저 곱셈 계층을 구현해보자


***MultiLayer.py***
```python
class MultiLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out
    
    def backward(self, dout):
        dx = self.y * dout
        dy = self.x * dout
        
        return dx, dy
```

__init__() 에서는 인스턴스 변수인 x와 y를 초기화하고 이 두 변수는 순전파 시의 입력값을 유지하기 위해서 사용한다. forward()에서는 x와 y를 인수로 받고 ㄷ두 값을 곱해 반환하며 반면 backward()에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 서로 바꾸어 곱한 후 하류로 흘린다.

***AddLayer.py***
```python
class AddLayer:
    def __init__(self):
        pass
    
    def forward(self, x,  y):
        self.x = x
        self.y = y
        out = x + y
    
        return out
    
    def backward(self, dout):
        dx = dout
        dy = dout
    
        return dx, dy
```

덧셈 계층에서는 초기화가 필요없으니 __init__()에서는 아무 일도 하지않는다. 덧셈 계층의 forward()에서는 입력받은 두 인수 x, y를 더해서 반환하며 backward()에서는 상류에서 내려온 미분 값을 그대로 하류로 흘린다.

위에서 구현한 AddLayer와 MultiLayer를 사용하여 아래의 계산 그래프를 파이썬으로 구현해보자

![img5-8](/assets/images/badak/fig%205-17.png)

***buy_apple_orange.py***
```python
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()


apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)
price = mul_tax_layer.forward(all_price, tax)

print(price)

dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)
dornage, dorange_num = mul_orange_layer.backward(dorange_price)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print(dapple_num, dapple, dornage, dorange_num, dtax)
```

## 3.3 활성화 함수 계층 구현

앞에서 배운 3.1과 3.2의 내용을 기반으로 활성화 함수인 ReLU와 Sigmoid 계층을 구현해보자

### ReLU 계층

우선 활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.

$$

y =
\begin{cases}
x,\;if\;x>0\\
0,\;else
\end{cases}

$$

또한 역전파에 사용되는 x에 대한 y의 미분은 아래와 같다.

$$

\frac{dy}{dx} =
\begin{cases}
1,\;if\;x>0\\
0,\;else
\end{cases}

$$

위 식과 같이 순전파 떄의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘리며 순전파 때 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않ㄷ는다. 계산그래프로는 아래와 같이 나타낼 수 있다.

![img5-9](/assets/images/badak/fig%205-18.png)

위의 수식을 기반으로 구현을 해보자

***ReLU.py***
```python
class ReLU:
    def __init__(self):
        self.mask = None
    
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

ReLu 클래스는 mask라는 인스턴스 변수를 가진다. mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0이하인 인덱스는 True, 그 외는 False로 유지한다. 이를 기반으로 순전파 때의 입력 값이 0이하면 역전파 때의 값이 0으로 0을 초과 할때는 그 값을 그대로 유지하여 하류로 흘리게 된다.

### Sigmoid 계층

시그모이드 함수의 수식은 아래와 같다.

$$

y = \frac{1}{1+e^{-x}}

$$

또한 역전파에 사용되는 x에 대한 y의 미분은 아래와 같다.

$$

\frac{dy}{dx} = \frac{e^{-x}}{1+2e^{-x}+e^{-2x}}\\
　　　　　　　　\\
　　　　　　　　= \frac{1}{1+e^{-x}}\cdot(\frac{1+e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}) \\
　　　　　　　　\\= y\cdot(1-y)

$$

이를 계산 그래프로 나타내면 아래와 같다.

<center>

![img5-11](/assets/images/badak/fig%205-22.png)

</center>

## 3.4 Affnie 및 Softmax-with-Loss 계층 구현

신경망의 순전파에서는 가중치 신호의 총합을 계산하기 떄문에 행렬곱을 사용한다. 예를들어 X, W, B의 각 형상이 (2,), (2, 3), (3,)인 다차원 배열일 때 뉴런의 가중치의 합은 Y = np.dot(X, W) + B와 같이 계산이 되며 그리고 이 Y를 활성화 함수로 변환해 다음 층으로 전파하는 것이 신경망 순전파의 흐름이다. 계산 그래프로 나타내면 아래와 같다.


<center>

![img5-12](/assets/images/badak/fig%205-24.png)

</center>

지금까지는 계산 그래프에 노드 사이에 스칼라 값이 흘렀는 데 반해, 이 예에서는 행렬이 흐르고 있다. 이제 역전파에 대해서 생각해보자 행렬을 사용한 역전파도 행렬의 원소마다 전개해보았을 때 스칼라 값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다. 이를 계산 그래프의 역전파로 나타내보자

<center>

![img5-13](/assets/images/badak/fig%205-26.png)

</center>

지금까지 설명한 Affine 계층에 대하여 설명하였고 이 Affine 계층을 입력 데이터가 X 하나만이 아닌 데이터 N개를 묶어 순전파 하는 경우,  즉 배치용 Affine 계층을 구현해보자. 우선 계산 그래프는 다음과같다.

<center>

![img5-14](/assets/images/badak/fig%205-27.png)

</center>

이 계산 그래프를 토대로 배치용 Affine 계층을 python으로 구현해보자

***batch_AffineLayer.py***
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = x @ self.W
        
        return out
    
    def backward(self, dout):
        self.dW = self.x.T @ dout
        self.dB = np.sum(dout, axis = 0)
        dx = dout @ self.W.T

        return dx
```

### Softmax-with-Loss 계층

마지막으로 출력층에서 사용하는 소프트맥스 함수에 관해 설명한다. 앞에서 말했듯 소프트맥스 함수는 입력 값을 정규화하여 출력하게 되며 예를 들어 손글씨 숫자 인식에서의 Softmax 계층의 출력은 아래와 그림과 같다.

<center>

![img5-15](/assets/images/badak/fig%205-28.png)

</center>

손글씨 숫자는 가짓수가 10개이므로 Softmax게층의 입력은 10개가 된다. 이제 소프트맥스 계층을 구현할 것인데 손실 함수인 교차 엔트로피 오차도 포함하여 Softmax-with-Loss 계층이라는 이름으로 구현한다. 우선 계산그래프를 보자


<center>

![img5-16](/assets/images/badak/fig%205-29.png)

</center>

수식적으로 보았을 때 Cross-Entropy의 역전파는 쉽게 이해를 할 수 있었지만 소프트 맥스에서의 역전파를 이해하는데에는 시간이 걸렸던 것 같다. 하지만 계산그래프로 풀었을 때에는 쉽게 이해를 할 수 있었다.

Cross-Entropy의 역전파를 계산 그래프로 나타내면 다음과 같다. 위에서 배운 편미분과 합성 함수의 미분을 생각해보면 쉽게 풀이를 할 수 있다. 

<center>

![img5-17](/assets/images/badak/fig%20a-4.png)

</center>

여기서 구해진 $ -\frac{t}{y} $ 를 소프트 맥스 함수에 전파해준다. 다음은 Softmax의 계산 그래프를 살펴보자

<center>

![img5-18](/assets/images/badak/fig%20a-4(6).png)

</center>

 / 노드에서 + 노드로 흐르는 $\frac{1}{S}(t_1 + t_2 + t_3)$ 가 $ \frac{1}{S} $ 가 된 이유는 $t$ 가 원 핫 벡터이기 때문에 전체 원소의 합이 1이 되기 때문에 $\frac{1}{S} \cdot 1 = \frac{1}{S} $ 이 된 것을 확인할 수 있다. 나머지 부분은 위에서 배운 노드이기 때문에 풀이하는데에는 문제가 없음을 알 수 있다.

 신기하게도 소프트맥스의 손실 함수로 교차 엔트로피 오차를 사용하니 역전파가 $(y_1 - t_1, y_2-t_2, y_3-t_3)$가 된 것을 확인할 수 있으며 이는 말끔함은 우연이 아닌 교차 엔트로피 오차라는 함수가 그렇게 설계된 함수이기 때문이다.

 마지막으로 계산그래프를 통해 알게된 SoftmaxwithLoss 계층의 순전파와 역전파를 토대로 구현을 해보자

***SoftmaxwithLoss.py***
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy(self.y, self.t)
        
        return self.loss
    
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        
        return dx
```

## 3.5 오차역전파법을 사용한 학습 구현하기

우선 학습을 하기전 학습을 시킬 모델을 정의해보자 MNIST 손글씨 이미지를 사용할 예정이기 떄문에 입력사이즈는 28 * 28 = 784를 사용하고 출력사이즈는 10가지의 분류를 하기위해 10을 사용한다. 또한 중간의 은닉층을 두어 입력이미지에서 특징을 뽑은 뒤 분류를 하는 모델을 설계한다. 가중치는 평균이 0이고 표준편차가 1인 정규분포로 초기화를 하고 활성함수로는 ReLU 출력층에 소프트맥스와 교차 엔트로피 오차를 사용하여 오차역전파를 계산한다. 이러한 모델을 다음과 같이 구현하였다.

***TwoLayerNet.py***
```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
        weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1:
            t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def gradient(self, x, t):
        self.loss(x, t)

        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db

        return grads
```

이제 모델을 설계하였으니 마지막으로 학습을 시켜보자 

***backprop_train.py***
```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

plt.subplot(1, 2, 1)
plt.plot(train_acc_list)
plt.title("Train Acc")
plt.subplot(1, 2, 2)
plt.title("Test Acc")
plt.plot(test_acc_list)
plt.show()
```

```bash
train acc, test acc | 0.0967, 0.1009
train acc, test acc | 0.9035, 0.9091
train acc, test acc | 0.9242, 0.9272
train acc, test acc | 0.9357666666666666, 0.9362
train acc, test acc | 0.9465333333333333, 0.9437
train acc, test acc | 0.9518, 0.949
train acc, test acc | 0.9566333333333333, 0.953
train acc, test acc | 0.9625333333333334, 0.955
train acc, test acc | 0.9658, 0.9599
train acc, test acc | 0.9668, 0.9603
train acc, test acc | 0.9668333333333333, 0.962
train acc, test acc | 0.97295, 0.9657
train acc, test acc | 0.9747833333333333, 0.9657
train acc, test acc | 0.9759833333333333, 0.9678
train acc, test acc | 0.9776333333333334, 0.9688
train acc, test acc | 0.9794, 0.9703
train acc, test acc | 0.9794, 0.9689
```

<center>

![img5-19](/assets/images/badak/Figure_1.png)

</center>