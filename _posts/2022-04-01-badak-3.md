---
title:  "오차역전파" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-04-01
last_modified_at: 2022-04-01
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 3. 오차역전파

앞에서 배운 수치미분으로 신경망을 학습시키는 것에대해서 이론을 살펴본 뒤 구현을 
해보았다. 이번 장에서는 시간은 오래 걸리지만 구현이 상대적으로 쉬운 수치미분이 아닌 고속으로 신경망을 학습시킬 수 있는 오차역전파에 대해서 살펴볼 예정이다.

## 3.1 오차역전파란?

오차역전파란 오차를 역으로 전파를 한다 라는데에서 지어진 이름이며 신경망의 가중치 매개변수를 고속으로 계산할 수 있는 방법이다. 이번 포스팅에서는 오차역전파를 계산 그래프를 통해 설명을 할 것이며 보통의 오차역전파를 이해하는데는 두 가지 방법이 있다. 하나는 수식을 통한 것이고 두 번째 방법은 계산 그래프를 통한 방법이다. 일반적으로는 수식을 통한 방법을 사용하지만 오차역전파법을 시각적으로 보며 이해를 하는 시간을 가질 것이다.


### 계산그래프

계산 그래프는 계산 과정을 그래프로 나타낸 것이다. 여기에서 그래프는 자료구조에서의 그래프를 말하며 복수의 노드와 에지로 표현된다. 계산 그래프는 계산 과정을 노드와 화살표로 표현하며 노드는 원으로 표기하고 원 안에 연산내용을 적는다. 또, 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다.

아래 문제의 풀이를 계산그래프로 나타내보자

 사과 1개의 가격은 100원이고 2개를 산다고 하였을 떄 가격은 얼마인가? 단 소비세 10%가 부과된다.

![img5-1](/assets/images/badak/fig%205-2.png)

다음 문제이다. 사과 1개의 가격은 100원이고 귤은 150원이다. 사과 2개, 귤 3개를 구매한다고 하였을 때 가격은 얼마인가? 단 소비세가 10%가 부과된다.

![img5-2](/assets/images/badak/fig%205-3.png)

위 두문제의 과정과 같이 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행한다.

1. 계산 그래프를 구성한다.
2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.

여기서 2. 의 과정을 순전파라고 하며 반대 방향으로 즉 오른쪽에서 왼쪽으로 전파하는 경우를 역전파라고 칭한다. 역전파는 이후 미분을 계산할 때 중요한 역할을 한다.

지금까지 계산 그래프를 사용하여 두 문제를 풀어보았다. 그래서 이 계산 그래프의 장점이 무엇인가 묻는다면 <u>국소적 계산</u>이라고 할 수 있다. 여기서 말하는 국소적 계산이란 아래 그림을 예를들어 설명하겠다.

![img5-3](/assets/images/badak/fig%205-4.png)

여러 식품을 구입하여 총 4,000원이 되었고 거기에 사과를 2개 구입함으로써 총 4,200원이 되었다. 여기서 국소적 계산이란 가령 사고와 그 외의 물품 값을 더하는 계산은 4,000원이 어떻게 계산되었느냐와는 상관없이, 단지 그 숫자를 더하기만 하면된다는 뜻이다. 각 노드는 자신과 관련한 계산외에는 신경쓸 것이 없다.

또한 계산 그래프의 다른 장점으로는 역전파를 통해 <u>미분</u>을 효율적으로 계산할 수 있다는 것이다. 이 또한 예를들어 설명하자면 위 사과 2개를 구매하는 문제에서 사과의 가격이 오르면 최종 금액에 어떤 영향을 미치는지에 대해 계산한다고 가정했을 때 이는 사과 가격에 대한 최종 금액의 미분 을 구하는 문제에 해당한다. 

![img5-4](/assets/images/badak/fig%205-5.png)

이 예에서는 역전파는 오른쪽에서 왼쪽으로 미분 값을 전달한다. 이 결과로 부터 사과 가격에 대한 지불금액의 미분 값은 2.2라고 할 수 있으며 사과가 1원이 올랐을 때 최종 금액은 2.2원이 오른다는 것이다.

### 연쇄법칙

그동안 해온 계산 그래프이 순전파는 계산 결과를 왼쪽에서 오른쪽으로 전달하였다. 이 순서는 평소의 하는 방식과 같아 자연스럽게 느껴졌을 것이다. 한편 역전파는 국소적인 미분을 순방향과는 반대인 오른쪽에서 왼쪽으로 전달한다. 또한 이 국소적 미분을 전달하는 원리는 연쇄법칙에 따른 것이며 따라서 연쇄 법칙을 설명하고 그것이 계산 그래프 상의 역전파와 같다는 사실을 설명한다. 아래는 $ y=f(x) $ 라는 계산의 역전파를 나타낸 것이다.

![img5-5](/assets/images/badak/fig%205-6.png)

위 그림과 같이 역전파의 계산 절차는 신호 E에 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다. 여기에서 말하는 국소적 미분은 순전파 때의 $ y=f(x) $ 계산의 미분을 구한다는 것이며 이는 x에 대한 y의 미분을 구한다는 뜻이다. 가령 $y=f(x)=x^2$라면 $\frac{dy}{dx}=2x$가 된다. 그리고 이 국소적 미분을 상류에서 전달된 E에 곱해 앞쪽 노드로 전달하는 것이다.

이것이 역전파의 계산 순서인데, 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심이다. 왜 이런 일이 가능한가는 연쇄법칙의 원리로 설명할 수 있다.

연쇄 버빅을 설명하려면 우선 합성 함수의 이야기부터 시작한다. 합성 함수란 여러 함수로 구성된 함수이다. 예를 들어 $z=(x+y)^2$의 식은 두개의 식으로 구성된다.

$$ z=t^2 $$

$$ t=x+y $$

합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다라는 것이 연쇄 법칙의 원리이며 이 또한 예로 설명하자면 $\frac{dz}{dx}$는 $\frac{dz}{dt}$ 와
$\frac{dt}{dx}$ 의 곱으로 설명할 수 있다는 것이다. 수식으로는 다음과 같이 쓸 수 있다.

$$ \frac{dz}{dx} = \frac{dz}{dt} \frac{dt}{dx} $$

위 식에서 $ dt $ 는 약분 가능하며 미분 $\frac{dz}{dx}$ 를 구해보자 

$$ \frac{dz}{dt} = 2t $$
$$ \frac{dt}{dx} = 1 $$

$$ \frac{dz}{dx} = 2t \cdot 1 = 2(x+y) \cdot 1$$

위와 같은 계산을 통하여 합성 함수의 미분을 하는 과정을 연쇄 법칙을 통하여 할 수 있게 되었으며 이 연쇄법칙 계산을 계산 그래프로 나타내면 아래와 같다.

![img5-6](/assets/images/badak/fig%205-8.png)

위 그림과 같이 계산 그래프의 역전파는 오른쪽에서 왼쪽으로 신호를 전달하며 역전파의 계산 절차에서는 노드로 들어온 입력신호에 그 노드의 국소적 미분을 곱한 후 다음 노드로 전달한다. 이 경우에서는 입력이 1이며 ($\frac{dz}{dz}$ 이므로) 국소적 미분인 $\frac{dz}{dt}$ 를 곱하고 다음 노드로 넘긴다.(순전파 시 입력이 t이고 출력이 z이므로 이 노드에서 미분은 $dz/dt$가 된다.) 또한 $\frac{dz}{dt} \cdot \frac{dz}{dz}$ 에 t에 대한 x의 미분인 $\frac{dt}{dx}$ 를 곱하면 $ \frac{dz}{dx} = \frac{dz}{dz} \frac{dz}{dt} \frac{dt}{dx} $ 가 되므로 즉, 역전파가 하는 일은 연쇄 법칙과 원리가 같다는 것을 알 수 있다.

## 3.2 단순한 계층 구현하기

모든 계층은 forward()와 backward()라는 공통의 메서드를 갖도록 구현한다. forward()는 순전파 backward()는 역전파를 처리한다.

그럼 먼저 곱셈 계층을 구현해보자


***MultiLayer.py***
```python
class MultiLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out
    
    def backward(self, dout):
        dx = self.y * dout
        dy = self.x * dout
        
        return dx, dy
```

__init__() 에서는 인스턴스 변수인 x와 y를 초기화하고 이 두 변수는 순전파 시의 입력값을 유지하기 위해서 사용한다. forward()에서는 x와 y를 인수로 받고 ㄷ두 값을 곱해 반환하며 반면 backward()에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 서로 바꾸어 곱한 후 하류로 흘린다.

***AddLayer.py***
```python
class AddLayer:
    def __init__(self):
        pass
    
    def forward(self, x,  y):
        self.x = x
        self.y = y
        out = x + y
    
        return out
    
    def backward(self, dout):
        dx = dout
        dy = dout
    
        return dx, dy
```

덧셈 계층에서는 초기화가 필요없으니 __init__()에서는 아무 일도 하지않는다. 덧셈 계층의 forward()에서는 입력받은 두 인수 x, y를 더해서 반환하며 backward()에서는 상류에서 내려온 미분 값을 그대로 하류로 흘린다.

위에서 구현한 AddLayer와 MultiLayer를 사용하여 아래의 계산 그래프를 파이썬으로 구현해보자

![img5-8](/assets/images/badak/fig%205-17.png)

***buy_apple_orange.py***
```python
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()


apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)
price = mul_tax_layer.forward(all_price, tax)

print(price)

dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)
dornage, dorange_num = mul_orange_layer.backward(dorange_price)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print(dapple_num, dapple, dornage, dorange_num, dtax)
```

## 3.3 활성화 함수 계층 구현

앞에서 배운 3.1과 3.2의 내용을 기반으로 활성화 함수인 ReLU와 Sigmoid 계층을 구현해보자

### ReLU 계층

우선 활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.

$$

y =
\begin{cases}
x,\;if\;x>0\\
0,\;else
\end{cases}

$$

또한 역전파에 사용되는 x에 대한 y의 미분은 아래와 같다.

$$

\frac{dy}{dx} =
\begin{cases}
1,\;if\;x>0\\
0,\;else
\end{cases}

$$

위 식과 같이 순전파 떄의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘리며 순전파 때 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않ㄷ는다. 계산그래프로는 아래와 같이 나타낼 수 있다.

![img5-9](/assets/images/badak/fig%205-18.png)

위의 수식을 기반으로 구현을 해보자

***ReLU.py***
```python
class ReLU:
    def __init__(self):
        self.mask = None
    
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

ReLu 클래스는 mask라는 인스턴스 변수를 가진다. mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0이하인 인덱스는 True, 그 외는 False로 유지한다. 이를 기반으로 순전파 때의 입력 값이 0이하면 역전파 때의 값이 0으로 0을 초과 할때는 그 값을 그대로 유지하여 하류로 흘리게 된다.

### Sigmoid 계층

시그모이드 함수의 수식은 아래와 같다.

$$

y = \frac{1}{1+e^{-x}}

$$

또한 역전파에 사용되는 x에 대한 y의 미분은 아래와 같다.

$$

\frac{dy}{dx} = \frac{e^{-x}}{1+2e^{-x}+e^{-2x}}\\
　　　　　　　　\\
　　　　　　　　= \frac{1}{1+e^{-x}}\cdot(\frac{1+e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}) \\
　　　　　　　　\\= y\cdot(1-y)

$$

이를 계산 그래프로 나타내면 아래와 같다.

<center>

![img5-11](/assets/images/badak/fig%205-22.png)

</center>

## 5.4 Affnie 및 Softmax-with-Loss 계층 구현

신경망의 순전파에서는 가중치 신호의 총합을 계산하기 떄문에 행렬곱을 사용한다. 예를들어 X, W, B의 각 형상이 (2,), (2, 3), (3,)인 다차원 배열일 때 뉴런의 가중치의 합은 Y = np.dot(X, W) + B와 같이 계산이 되며 그리고 이 Y를 활성화 함수로 변환해 다음 층으로 전파하는 것이 신경망 순전파의 흐름이다. 계산 그래프로 나타내면 아래와 같다.


<center>

![img5-12](/assets/images/badak/fig%205-24.png)

</center>

지금까지는 계산 그래프에 노드 사이에 스칼라 값이 흘렀는 데 반해, 이 예에서는 행렬이 흐르고 있다. 이제 역전파에 대해서 생각해보자 행렬을 사용한 역전파도 행렬의 원소마다 전개해보았을 때 스칼라 값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다. 이를 계산 그래프의 역전파로 나타내보자

<center>

![img5-13](/assets/images/badak/fig%205-26.png)

</center>

지금까지 설명한 Affine 계층에 대하여 설명하였고 이 Affine 계층을 입력 데이터가 X 하나만이 아닌 데이터 N개를 묶어 순전파 하는 경우,  즉 배치용 Affine 계층을 구현해보자. 우선 계산 그래프는 다음과같다.

<center>

![img5-14](/assets/images/badak/fig%205-27.png)

</center>

이 계산 그래프를 토대로 배치용 Affine 계층을 python으로 구현해보자

***batch_AffineLayer.py***
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = x @ self.W
        
        return out
    
    def backward(self, dout):
        self.dW = self.x.T @ dout
        self.dB = np.sum(dout, axis = 0)
        dx = dout @ self.W.T

        return dx
```

### Softmax-with-Loss 계층

마지막으로 출력층에서 사용하는 소프트맥스 함수에 관해 설명한다. 앞에서 말했듯 소프트맥스 함수는 입력 값을 정규화하여 출력하게 되며 예를 들어 손글씨 숫자 인식에서의 Softmax 계층의 출력은 아래와 그림과 같다.

<center>

![img5-15](/assets/images/badak/fig%205-28.png)

</center>

손글씨 숫자는 가짓수가 10개이므로 Softmax게층의 입력은 10개가 된다. 이제 소프트맥스 계층을 구현할 것인데 손실 함수인 교차 엔트로피 오차도 포함하여 Softmax-with-Loss 계층이라는 이름으로 구현한다.

