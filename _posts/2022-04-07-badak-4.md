---
title:  "학습 관련 기술들" 

categories:
  - Basic
tags:
  - [Programming, Python]

toc: true
toc_sticky: true

date: 2022-04-07
last_modified_at: 2022-04-07
---

밑바닥부터 시작하는 딥러닝 책을 공부한 내용을 토대로 작성한 글입니다.

<br>

# Chapter 4. 학습 관련 기술

전 글에서는 오차 역전파에 대한 이론과 더불어 구현함으로써 신겸망을 학습시켜보았다. 이번 글에서는 학습되는 가중치 매개변수의 최적값을 탐색하는 방법과 가중치 매개변수의 초깃값 설정 및 하이퍼파라미터의 설정 방법, 오버피팅의 대응책인 가중치 감소와 드롭아웃, 배치정규화등에 대해서 다룰 생각이다. 

## 4.1 최적화

신경망 학습시키는 지표는 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다. 이는 곧 매개변수의 최적값을 찾는 문제이며 이러한 문제를 푸는 것을 최적화라고 한다. 매개변수 공간은 매우 넓고 복잡해서 최적의 값을 찾는 것은 매우 어려운 문제이다. 지금까지의 방법으로는 매개변수의 기울기를 구해 기울어진 방향으로 매개변수 값을 갱신하는 일을 몇번이고 반복해서 점점 최적의 값에 다가가는 방법인 확률적 경사 하강법(SGD)를 사용하였다. SGD도 문제에 따라서는 좋은 방법이 될 수 있지만 SGD로 해결이 되지않는 문제를 풀기위해서 SGD와는 다른 최적화 기법을 소개한다.

### SGD

우선 다른 최적화 기법을 소개하기 전 먼저 SGD를 복습해보자 수식으로는 다음과 같이 쓸 수 있다.

$$ W = W - \lambda \frac{dL}{dW} $$

여기에서 $W$
는 갱신할 가중치 매개변수를 말하며
$\frac{dL}{dW}$
는 $W$
에 대한 손실 함수의 기울기를 말한다. 또한
$\lambda$
는 학습률을 의미한다. 식에서 보면 알 수 있듯이 SGD는 기울진 방향으로 일정 거리만 가겠다는 단순한 방법이다. 이를 파이썬으로 구현하면 아래와같다.

```python
class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

이러한 SGD는 직관적이며 구현도 쉽게 할 수 있지만, 문제에 따라서는 비효율적일 때가 있다. SGD를 토아혀 아래 함수의 최소값을 구하는 문제를 생각해보자

$$
f(x,y) = \frac{1}{20}x^2 + y^2
$$

이 함수는 기울기는 아래의 모양과 같다.

![img4-1](/assets/images/badak/fig%206-2.png)

이 기울기는 y축 방향은 크고 x축 방향은 작다는 것이 특징이다. 또한 함수
$f(x,y)$
의 최솟값이 되는 장소는 (0, 0)임에도 불구하고 그림에서 보여주는 기울기의 대부분은 (0, 0)을 가르키지 않는 다는 것이다. 또한 이 함수에 SGD를 적용한 결과는 아래와 같다.

![img4-2](/assets/images/badak/fig%206-3.png)

SGD는 위와 같이 심하게 굽이진 움직임을 보이며 상당히 비효율적인 움직임인 것을 알 수 있으며 SGD가 지그재그로 탐색하는 근본 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리키기 때문이다. 즉, SGD의 단점은 비등방성 함수에서는 탐색경로가 비효율적이라는 것이다. 

비등방성 함수 - 비등방성이란 방향에 따라서 물리적 <u>성질</u>이 바뀌는 것을 의미함. 비등방성 함수는 앞에 언급한 성질이 기울기의 성질이라고 생각하면 된다. 즉, 비등방성 함수는 특정한 지점(좌표)에서 기울기의 성질이 변하는 함수를 말한다.

### Momentum

<u>Momentum</u>은 '운동량'을 뜻하는 단어로, 물리와 관계가 있다. 모멘텀 기법은 수식으로는 다음과 같이 쓸 수 있다.

$$
v = \alpha v - \gamma \frac{dL}{dW}
$$

$$
W = W + v
$$

SGD와 같이 여기에서도 
$W$
는 가중치 매개변수,
$\frac{dL}{dW}$
는 $W$
에 대한 손실 함수의 기울기
$\gamma$
는 학습률을 뜻한다. 
하지만 여기서
$v$
라는 새로운 변수가 나오는데, 이는 물리에서 말하는 속도에 해당한다. 위 식을 풀이하자면 기울기 방향으로 힘을 받아 물체가 가속된다는 물리법칙을 나타낸며 또, 식의
$\alpha$ 
항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다. 물리에서의 마찰이나 공기 저항에 해당한다고 생각하면 된다. 이를 구현하면 아래와 같다.

```python
class Momentum:
    def __init__(self, lr = 0.01, momentum = 0.9):
        self.lr = lr
        self.momentum = 0.9
        self.v = None
    
    def update(self, param, grads):
        if self.v is None:
            self.v = {}
            for key, val in param.items():
                self.v[key] = np.zeros_like(val)
        
        for key in param.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
            param[key] += self.v[key]
```

이제 모멘텀을 통해 최적화 문제를 풀 경우 결과는 다음과 같다.

![img4-3](/assets/images/badak/fig%206-5.png)

확실히 SGD와 비교하였을 때 지그재그의 정도가 덜한 것을 확인할 수 있고 이는 x축의 힘이 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문이다. 거꾸로 y축의 힘은 크지만 위아래로 번갈아받아서 상충하여 y축의 방향의 속도는 안정적이지 않은 것을 확인할 수 있다.

### Adagrad

신경망 학습에서는 학습률 값이 중요하다. 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다. 이 학습률을 정하는 효과적 기술로 <u>학습률 감소</u>가 있다. 이는 학습을 진행하면서 학습률을 줄여가는 방법이다. 처음에는 크게 학습하다가 조금씩 작게 학습한다는 얘기로, 실제 신경망 학습에 자주 쓰인다.

학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 전체의 학습률을 일괄적으로 낮추는 것이다. 하지만 이를 더욱 발전시킨 것이 AdaGrad이다. AdaGrad는 매개변수 각각에 맞는 맞춤형 학습률을 만들어준다.

AdaGrad의 갱신 방법은 수식으로는 다음과 같다.

$$
h = h + \frac{dL}{dW} \cdot \frac{dL}{dW}
$$

$$
W = W - \gamma \frac{1}{\sqrt{h}+\epsilon} \frac{dL}{dW}
$$

여기에서 새로
$h$
라는 변수가 등장하는데
$h$
는 식에서 보듯 기존 기울기 값을 제곱하여 계속 더해준다.(여기서 
$\cdot$
기호는 행렬의 원소곱을 뜻한다.) 그리고 매개변수를 갱신할 때
$\frac{1}{\sqrt{h}}$
를 곱하여 학습률을 조정한다. 매개변수의 원소 중에서 많이 움직인 원소는 학습률이 낮아진다는 뜻이며 다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용됨을 뜻한다.

그럼 AdaGrad의 구현을 살펴보자.

```python
class AdaGrad:
    def __init__(self, lr = 0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
            # 1e-7을 더해줌으로써 divide by zero 문제를 해결한다.
```

이제 AdaGrad를 사용해서 최적화 문제를 풀어보자

![img4-4](/assets/images/badak/fig%206-6.png)


그림을보면 최솟값을 향해 효율적으로 움직이는 것을 알 수 있다. y축 방향은 기울기가 커서 처음에는 크게 움직이지만, 그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정된다. 그래서 y축 방향으로의 갱신 강도가 빠르게 약해지고 지그재그 움직임이 줄어듦을 알 수 있다.

### RMSProp

위의 AdaGrad는 과거의 기울기를 제곱하여 계속더해간다. 그래서 학습을 진행할수록 갱신 강도가 약해지게된다. 실제로 무한히 계속학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 되는데 이문제를 개선한 기법으로서 RMSProp이라는 방법이 있다. RMSProp은 과거의 모든 기울기를 균일하게 더해가는 것이 아닌, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. 이를 지수이동평균이라 하여 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다. 수식으로 표현하면 다음과 같다.

$$
h = \alpha h + (1-\alpha)(\frac{dL}{dW})^2
$$

$$
W = W - \gamma \frac{1}{\sqrt{h+\epsilon}} \frac{dL}{dW}
$$

위 변수들은 대부분 위에서 사용된 것들과 같으며 변경된 사항인
$\alpha$
의 경우 예전의 값을 
$\alpha$
만큼 참조하고 새로운 가중치에 대해서는
$(1-\alpha)$
만큼 참조를 하여 갱신을 하기위해 사용하는 변수이다. 또한
$\epsilon$
은 Devide by 0 문제를 해결하기 위해 사용되는 변수이다. 파이썬으로 하는 구현은 아래와 같다.

```python
class RMSProp:
    def __init__(self, lr = 0.01, p = 0.99):
        self.lr = lr
        self.p = p
        self.h = None
    
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        
        for key in params.keys():
            self.h[key] = self.p * self.h[key] + (grads[key] * grads[key] * (1 - self.p))
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

구현하여 실행시킨 결과는 아래와 같은데 AdaGrad와 같이 최솟값을 향해 효율적으로 움직이며 갱신의 횟수가 많지 않기때문에 AdaGrad와 비슷한 움직임을 보인다.

![img4-5](/assets/images/badak/rmsprop-gray.png)


### Adam

Momentum은 공이 그릇 바닥을 구르는 듯한 움직임을 보였다. AdaGrad는 매개변수의 원소마다 적응적으로 갱신 정도를 조정했다. 이 두 기법을 활용하여 만든 기법이 Adam이다.

Adam은 2015년에 제안된 새로운 방법이며 그 이론은 다소 복잡하지만 직관적으로는 Momentum과 AdaGrad를 융합한듯한 방법이다. 수식으로는 아래와 같다.

$$
h = \beta_1h + (1-\beta_1)\frac{dL}{dW}
$$

$$
v = \beta_2v + (1-\beta_2)(\frac{dL}{dW})^2
$$

$$
\hat{h} = \frac{h}{1-\beta_1^t}
$$

$$
\hat{v} = \frac{v}{1-\beta_2^t}
$$

$$
W = W - \frac{\gamma}{\sqrt{\hat{v}}+\epsilon} \hat{h}
$$

$ h, v $
의 경우 위에서 살펴본 식들과 비슷한 점을 느낄수 있지만 
$ \hat{h}, \hat{v} $
는 왜 이렇게 구성이 되는지 의아할 수도 있다. 
그 이유는
$ h, v $
를 0으로 가중치를 초기화하면 초기 가중치들의 값이 0으로 편향되는 경향이 있으며 decay rate가 1에 가까울 수록 그 경향은 더욱 심해지게된다. 그러므로 편향을 잡아주기 위해 bias-corrected를 사용한다. Adam을 파이썬으로 구현하고 실행시킨 결과는 다음과 같다.

```python
class Adam:
    def __init__(self, lr = 0.01, beta1 = 0.9, beta2 = 0.99):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.h = None
        self.v = None
        self.iter = 0
    
    def update(self, params, grads):
        if self.h is None:
            self.h, self.v = {}, {}

            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t = self.lr * np.sqrt(1. - self.beta2 ** self.iter) / (1. - self.beta1 ** self.iter)

        for key in params.keys():
            self.h[key] += (1 - self.beta1) * (grads[key] - self.h[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])

            params[key] -= lr_t * self.h[key] / (np.sqrt(self.v[key] + 1e-7))

```

![img4-6](/assets/images/badak/fig%206-7.png)

Adam의 갱신 과정도 모멘텀과 비슷한 패턴인 그릇 바닥을 구르듯 움직이는 듯한 몹습을 보이는데 모멘텀 때보다 공의 좌우 흔들림이 적은 것을 확인할 수 있다. 이는 학습의 갱신 강도를 적응적으로 조정함으로써 얻는 혜택이다.

![img4-7](/assets/images/badak/fig%206-8.png)

여태까지 다섯 가지 기법의 결과를 비교한 사진이다. RMSProp은 AdaGrad와 거의 흡사한 패턴을 보였기에 제외하였다. 위 사진을 보면 사용한 기법에 따라 갱신 경로가 다른 것을 확인할 수 있으며 이 중에서는 AdaGrad가 가장 나은 것을 확인할 수 있다. 하지만 결과는 풀어야 할 문제가 무엇이냐에 따라 달라지므로 주의해야할 필요가 있고 하이퍼파라미터를 어떻게 설정하느냐에 따라서도 결과가 바뀌기도한다. 그러므로 상황을 고려하여 여러가지로 시도를 해볼필요가 있다.

## 4.2 가중치의 초기값

신경망 학습에 있어 특히 중요한 것중 하나는 가중치의 초깃값이다. 가중치의 초깃값을 무엇으로 하느냐에 따라 학습의 결과는 천차만별이 될 수 있기 때문이다. 그러므로 가중치의 초기값에 대한 여러가지 실험을 통하여 가중치의 초기값을 설정하는 방법을 소개한다.

### 초기값 0 설정
가중치를 작게 만들기 위해서는 초기값도 최대한 작은 값에서 시작하는 것이 정곱법이다. 사실 지금까지의 가중치의 초기값은 평균이 0 표준편차가 0.01인 정규분포애서 생성되도록 하였다. 그렇다면 가중치의 초기값을 전부 0으로 하면 어떻게 될까? 답은 올바른 학습이 이루어지지 않는다는 것이다.

초기값을 0으로 설정 또는 가중치를 균일한 값으로 설정할 경우 오차역전파를 할 때 모든 가중치의 값이 똑같이 갱신되기 때문에 올바른 학습이 이루어지지 않게된다. 예를들어 첫번 째와 두번째 층의 가중치가 0일 경우 순전파 일때 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모두 같은 값이 전달되게 되고 두 번째 층의 모든 뉴런에 같은 값이 입력된다. 두 번째 층의 모든 누련에 같은 값이 입력된다는 것은 역전파 때 두 번째 층의 가중치가 모두 똑같이 갱신된다는 것이고 그래서 가중치들은 같은 초기값에서 시작하고 갱신을 거쳐도 여전히 같은 값을 유지하는 것이고 이는 가중치를 여러 개를 갖는 의미를 사라지게 한다는 것이다. 이 가중치가 고르게 되어버리는 상황을 막기위해서는 초깃값을 무작위로 설정하여야한다.

### 은닉층의 활성화 값 분포

은닉층의 활성화 함수의 출력 데이터의 분포를 관찰하면 중요한 정보를 얻을 수 있다. 가중치의 초기값에 따라 은닉층의 활성화 값들이 어떻게 변화하는지 간단히 실험을 해보자 구체적으로는 Sigmoid, ReLU등의 활성화 함수들을 사용하여 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려본다. 실험 코드는 아래와 같다.

```python
import numpy as np
import matplotlib.pyplot as plt


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def ReLU(x):
    return np.maximum(0, x)


def tanh(x):
    return np.tanh(x)


input_data = np.random.randn(1000, 100)
node_num = 100
hidden_layer_size = 5
activations = {}

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    # w = np.random.randn(node_num, node_num) * 1
    # w = np.random.randn(node_num, node_num) * 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
     w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)

    a = np.dot(x, w)

    z = sigmoid(a)
    # z = ReLU(a)

    activations[i] = z

for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0:
        plt.yticks([], [])
    plt.hist(a.flatten(), 30, range=(0, 1))

plt.show()
```

우선 표준편차가 1인 정규분포와 Sigmoid 활성함수를 이용하여 실험을 진행하고 가중치 값의 히스토그램을 살펴보자

![img4-8](/assets/images/badak/fig%206-10.png)

각 층의 활성화 값들이 대부분 0과 1에 치우친 것을 확인할 수 있다. 여기서 사용된 활성함수인 Sigmoid는 출력이 0에 가까워지게 될수록 그 미분은 0에 다가가게 된다. 그래서 데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라지게되고 이러한 현상을 <u>기울기 소실(Gradient Vanishing)</u>이라고 한다. 층을 깊게하는 딥러닝에서는 기울기 소실은 더 심각한 문제가 될수있다. 이번에는 가중치의 표준편차를 0.01로 바꿔 같은 실험을 해보자

![img4-9](/assets/images/badak/fig%206-11.png)

이번에는 대부분의 값이 0.5에 집중된 것을 확인할 수 있다. 앞의 예와는 달리 0과 1에 치우치지 않아 기울기 소실 문제는 일어나지 않지만 활성화 값들이 치우쳤다는 것은 표환력 관점에서는 문제가 있는 것이다. 즉 가중치의 값들을 전부 같은 값으로 초기화 할때와 같이 다수의 뉴런이 거의 같은 값들을 출력하고 있으므로 뉴런을 여러 개로 한 의미가 없어진다는 것이다. 그래서 활성화 값이 치우치게되면 표현력을 제한한다는 관점에서 문제가 된다.

그러므로 각 층의 활성화 값은 적당히 고루 분포되어야하며 층과 층사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이뤄진다.

이어서 Xavier Glorot와 Yosha Bengio의 논문에서 권장하는 가중치 초기값인 Xavier 초기값을 사용해보자 
이 논문은 각 층의 활성화 값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 하였다. 그리고 앞 계층의 노드가 n개라면 표준편차가 
$\frac{1}{\sqrt{n}}$
인 분포를 사용하면 된다는 결론을 이끌었다. Xavier 초기값을 사용하면 앞층의 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 펴지게된다. 이제 Xavier 초깃값을 사용해 실험을 해보자

![img4-10](/assets/images/badak/fig%206-13.png)

Xavier 초깃값을 사용한 결과 층이 깊어지면서 형태가 다소 일그러지지만 앞에서 본 방식보다는 넓게 분포됨을 알 수 있다. 각층에 흐르는 데이터는 적당히 퍼져 있으므로 시그모이드 함수의 표현력도 제한받지 않고 학습이 효율적으로 이뤄질 것으로 기대된다.

하지만 Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과이며 Sigmoid 함수는 좌우 대칭이라 중앙 부근이 선형인 함수로 볼 수 있다. 그래서 Xavier 초깃값이 적당하지만 ReLU를 사용할 때는 ReLU에 특화된 초깃값을 이용하라고 권장한다. 이 특화된 초기값을 찾아낸 Kaimming He의 이름을 따 <u>He 초깃값</u>이라하고 He초깃값은 앞 계층의 노드가 n개일 때 표준편차가 
$\frac{2}{\sqrt{n}}$
인 정규분포를 사용한다. 활설함수를 ReLU로 하꼬 표준편차가 0.01인 정규분포, Xavier 초깃값, He초깃값 일 때의 실험 결과는 다음과 같다.

![img4-11](/assets/images/badak/fig%206-14.png)

결과를 보면 표준편차가 0.01 일 때의 각 층의 활성화 값들은 아주 작은 값들임을 확인할 수 있으며 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 떄 기울기 역시 작아진다는 뜻이다. 이는 중요한 문제이며 실제로도 학습이 거의 이뤄지지 않는다는 것이다.

이어서 Xavier 초깃값의 결과를 보면 층이 깊어지면서 치우침이 조금씩 커지게 되고 실제로 층이 깊어지면 활성화 값들의 치우침도 커지고 학습할 떄 기울기 소실문제를 야기한다.

마지막으로 He 초기값은 모든 층에서 균일하게 분포됨을 확인할 수 있고 층이 깊어져도 분포가 균일하게 유지되기에 역전파 때도 적절한 값이 나올 것으로 기대할 수 있다.

위의 실험을 결과로 활성 함수를 Sigmoid를 사용할 때는 Xavier 초깃값을 ReLU를 사용할 때는 He 초깃값을 사용하는 것이 좋다라는 것을 알 수 있다.